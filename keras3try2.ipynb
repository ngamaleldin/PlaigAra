{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 14:50:28.418535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731588628.429055  489330 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731588628.432265  489330 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, GlobalAveragePooling1D, Concatenate, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFLongformerModel,\n",
    "    LongformerConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=25)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=15)\n",
    "    plt.yticks(tick_marks, classes, fontsize=15)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j, i, format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=14\n",
    "        )\n",
    "\n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('FinalDatasetBalanced.csv')\n",
    "df['plagiarism_type'] = df['plagiarism_type'].factorize()[0]\n",
    "map_label = dict(enumerate(df['plagiarism_type'].factorize()[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    # Use keras.utils.set_random_seed for setting the seed\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "def convert_to_transformer_inputs(str1, str2, tokenizer, max_sequence_length, double=True):\n",
    "    def return_id(str1, str2, length):\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        input_masks = inputs[\"attention_mask\"]\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        \n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "\n",
    "    if double:\n",
    "        input_ids_1, input_masks_1, input_segments_1 = return_id(str1, None, max_sequence_length)\n",
    "        input_ids_2, input_masks_2, input_segments_2 = return_id(str2, None, max_sequence_length)\n",
    "\n",
    "        return [\n",
    "            input_ids_1, input_masks_1, input_segments_1,\n",
    "            input_ids_2, input_masks_2, input_segments_2\n",
    "        ]\n",
    "    else:\n",
    "        input_ids, input_masks, input_segments = return_id(str1, str2, max_sequence_length)\n",
    "        return [input_ids, input_masks, input_segments, None, None, None]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length, double=True):\n",
    "    input_ids_1, input_masks_1, input_segments_1 = [], [], []\n",
    "    input_ids_2, input_masks_2, input_segments_2 = [], [], []\n",
    "    \n",
    "    for _, instance in df[columns].iterrows():\n",
    "        str1, str2 = instance[columns[0]], instance[columns[1]]\n",
    "        ids_1, masks_1, segments_1, ids_2, masks_2, segments_2 = \\\n",
    "            convert_to_transformer_inputs(str1, str2, tokenizer, max_sequence_length, double=double)\n",
    "        \n",
    "        input_ids_1.append(ids_1)\n",
    "        input_masks_1.append(masks_1)\n",
    "        input_segments_1.append(segments_1)\n",
    "        input_ids_2.append(ids_2)\n",
    "        input_masks_2.append(masks_2)\n",
    "        input_segments_2.append(segments_2)\n",
    "\n",
    "    if double:\n",
    "        return [\n",
    "            np.asarray(input_ids_1, dtype=np.int32), \n",
    "            np.asarray(input_masks_1, dtype=np.int32), \n",
    "            np.asarray(input_segments_1, dtype=np.int32),\n",
    "            np.asarray(input_ids_2, dtype=np.int32), \n",
    "            np.asarray(input_masks_2, dtype=np.int32), \n",
    "            np.asarray(input_segments_2, dtype=np.int32)\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            np.asarray(input_ids_1, dtype=np.int32), \n",
    "            np.asarray(input_masks_1, dtype=np.int32), \n",
    "            np.asarray(input_segments_1, dtype=np.int32)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1702, 2) (730, 2)\n",
      "(1702,) (730,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['source_content', 'suspicious_content']], df['plagiarism_type'].values, \n",
    "    random_state=33, test_size=0.3\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# Import tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 16000\n",
    "MODEL_NAME = \"longformer-encdec-large-16384\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create sequences for train and test\n",
    "input_train = compute_input_arrays(\n",
    "    X_train, ['source_content', 'suspicious_content'], tokenizer, MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "input_test = compute_input_arrays(\n",
    "    X_test, ['source_content', 'suspicious_content'], tokenizer, MAX_SEQUENCE_LENGTH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_longformer():\n",
    "    set_seed(33)\n",
    "    opt = Adam(learning_rate=2e-5)\n",
    "    \n",
    "    config = LongformerConfig.from_pretrained(MODEL_NAME)\n",
    "    config.max_position_embeddings = 16000\n",
    "    config.attention_window = [256] * config.num_hidden_layers\n",
    "\n",
    "    class DualLongformerModel(keras.Model):\n",
    "        def __init__(self, config, model_name, num_labels):\n",
    "            super(DualLongformerModel, self).__init__()\n",
    "            self.longformer_model1 = TFLongformerModel.from_pretrained(\n",
    "                model_name, config=config, from_pt=True\n",
    "            )\n",
    "            self.longformer_model2 = TFLongformerModel.from_pretrained(\n",
    "                model_name, config=config, from_pt=True\n",
    "            )\n",
    "            self.global_pool = GlobalAveragePooling1D()\n",
    "            self.concat = Concatenate()\n",
    "            self.dense1 = Dense(64, activation='relu')\n",
    "            self.dropout = Dropout(0.2)\n",
    "            self.classifier = Dense(num_labels, activation='softmax')\n",
    "\n",
    "        def call(self, inputs):\n",
    "            id1, mask1, atn1, id2, mask2, atn2 = inputs\n",
    "            outputs1 = self.longformer_model1(\n",
    "                input_ids=id1,\n",
    "                attention_mask=mask1,\n",
    "                token_type_ids=atn1,\n",
    "                training=False\n",
    "            )\n",
    "            outputs2 = self.longformer_model2(\n",
    "                input_ids=id2,\n",
    "                attention_mask=mask2,\n",
    "                token_type_ids=atn2,\n",
    "                training=False\n",
    "            )\n",
    "            x1 = self.global_pool(outputs1.last_hidden_state)\n",
    "            x2 = self.global_pool(outputs2.last_hidden_state)\n",
    "            x = self.concat([x1, x2])\n",
    "            x = self.dense1(x)\n",
    "            x = self.dropout(x)\n",
    "            out = self.classifier(x)\n",
    "            return out\n",
    "\n",
    "    model = DualLongformerModel(config, MODEL_NAME, num_labels=len(map_label))\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        run_eagerly=True,    # Add this line\n",
    "        jit_compile=False    # Add this line\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adjusted the callback to save the model in .keras format\n",
    "class SaveModelCallback(Callback): \n",
    "    def __init__(self, save_path='longformer_checkpoints'):\n",
    "        super().__init__()\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_path = f'{self.save_path}/epoch_{epoch+1}'\n",
    "        os.makedirs(epoch_path, exist_ok=True)\n",
    "\n",
    "        # Save the model in the new .keras format\n",
    "        self.model.save(f'{epoch_path}/model_{epoch+1}.keras') \n",
    "        print(f'\\nSaved model to {epoch_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bart to instantiate a model of type longformer. This is not supported for all configurations of models and can yield errors.\n",
      "2024-11-14 14:51:08.700763: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1731588668.700885  489330 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2538 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "/home/msi/miniconda3/envs/longformer/lib/python3.11/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFLongformerModel: ['model.decoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.11.encoder_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.8.fc2.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.1.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.7.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.10.encoder_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.8.fc2.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.output.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.6.encoder_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.value.weight', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.encoder_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.6.fc2.weight', 'model.decoder.layers.1.fc2.weight', 'model.encoder.layers.3.self_attn.output.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.10.fc2.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.output.bias', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.4.fc1.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.8.self_attn.output.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.9.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.output.bias', 'model.decoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.11.fc2.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.10.final_layer_norm.weight', 'final_logits_bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.encoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.0.fc1.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.encoder.layers.5.fc1.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.11.encoder_attn.k_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.11.fc2.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.output.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.9.encoder_attn.k_proj.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.output.weight', 'model.encoder.layers.8.fc2.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.output.weight', 'model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.10.fc2.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.11.fc2.weight', 'model.decoder.layers.3.fc2.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.7.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.1.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.encoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.embed_positions.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.11.encoder_attn.q_proj.weight', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.3.fc2.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.value.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.0.fc2.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.9.fc2.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.6.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.7.fc1.weight', 'model.encoder.layers.11.fc1.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.7.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.output.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.10.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.9.encoder_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.3.fc2.bias', 'model.decoder.layers.9.encoder_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.10.fc2.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.10.self_attn.output.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.11.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.output.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.5.fc1.bias', 'model.decoder.layers.7.encoder_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.8.self_attn.output.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.8.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.10.encoder_attn.k_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.7.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.11.self_attn.output.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.9.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.10.encoder_attn.out_proj.weight', 'model.encoder.layers.11.fc1.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.11.fc1.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.output.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.8.encoder_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.7.fc2.bias', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.11.self_attn.longformer_self_attn.value.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.8.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.8.encoder_attn.out_proj.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.10.encoder_attn.k_proj.weight', 'model.encoder.layers.11.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.6.encoder_attn_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.output.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.6.encoder_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.7.fc1.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.output.weight', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc1.bias', 'model.decoder.layers.7.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.8.encoder_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.4.fc2.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.10.fc1.weight', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.9.fc1.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layernorm_embedding.weight', 'model.encoder.layers.8.fc2.bias', 'model.decoder.layers.6.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.8.fc1.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.8.encoder_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.output.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.11.encoder_attn.v_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.decoder.layers.9.encoder_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.11.encoder_attn_layer_norm.weight', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.key.bias', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.bias', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.6.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.output.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.9.encoder_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.11.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.10.encoder_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.1.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.9.fc2.weight', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.7.self_attn.output.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.output.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.value.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.value.weight', 'model.encoder.layers.8.fc1.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.9.fc2.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.output.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.output.weight', 'model.encoder.layers.2.fc1.bias', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.output.bias', 'model.decoder.layers.11.encoder_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.8.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.5.self_attn.output.weight', 'model.shared.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.7.fc2.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.4.fc2.bias', 'model.decoder.layernorm_embedding.bias', 'model.decoder.layers.8.encoder_attn.q_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing TFLongformerModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFLongformerModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.query_global.weight', 'encoder.layer.0.attention.self.query_global.bias', 'encoder.layer.0.attention.self.key_global.weight', 'encoder.layer.0.attention.self.key_global.bias', 'encoder.layer.0.attention.self.value_global.weight', 'encoder.layer.0.attention.self.value_global.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.query_global.weight', 'encoder.layer.1.attention.self.query_global.bias', 'encoder.layer.1.attention.self.key_global.weight', 'encoder.layer.1.attention.self.key_global.bias', 'encoder.layer.1.attention.self.value_global.weight', 'encoder.layer.1.attention.self.value_global.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.query_global.weight', 'encoder.layer.2.attention.self.query_global.bias', 'encoder.layer.2.attention.self.key_global.weight', 'encoder.layer.2.attention.self.key_global.bias', 'encoder.layer.2.attention.self.value_global.weight', 'encoder.layer.2.attention.self.value_global.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.query_global.weight', 'encoder.layer.3.attention.self.query_global.bias', 'encoder.layer.3.attention.self.key_global.weight', 'encoder.layer.3.attention.self.key_global.bias', 'encoder.layer.3.attention.self.value_global.weight', 'encoder.layer.3.attention.self.value_global.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.query_global.weight', 'encoder.layer.4.attention.self.query_global.bias', 'encoder.layer.4.attention.self.key_global.weight', 'encoder.layer.4.attention.self.key_global.bias', 'encoder.layer.4.attention.self.value_global.weight', 'encoder.layer.4.attention.self.value_global.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.query_global.weight', 'encoder.layer.5.attention.self.query_global.bias', 'encoder.layer.5.attention.self.key_global.weight', 'encoder.layer.5.attention.self.key_global.bias', 'encoder.layer.5.attention.self.value_global.weight', 'encoder.layer.5.attention.self.value_global.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query_global.weight', 'encoder.layer.6.attention.self.query_global.bias', 'encoder.layer.6.attention.self.key_global.weight', 'encoder.layer.6.attention.self.key_global.bias', 'encoder.layer.6.attention.self.value_global.weight', 'encoder.layer.6.attention.self.value_global.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.query_global.weight', 'encoder.layer.7.attention.self.query_global.bias', 'encoder.layer.7.attention.self.key_global.weight', 'encoder.layer.7.attention.self.key_global.bias', 'encoder.layer.7.attention.self.value_global.weight', 'encoder.layer.7.attention.self.value_global.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.query_global.weight', 'encoder.layer.8.attention.self.query_global.bias', 'encoder.layer.8.attention.self.key_global.weight', 'encoder.layer.8.attention.self.key_global.bias', 'encoder.layer.8.attention.self.value_global.weight', 'encoder.layer.8.attention.self.value_global.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.query_global.weight', 'encoder.layer.9.attention.self.query_global.bias', 'encoder.layer.9.attention.self.key_global.weight', 'encoder.layer.9.attention.self.key_global.bias', 'encoder.layer.9.attention.self.value_global.weight', 'encoder.layer.9.attention.self.value_global.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.query_global.weight', 'encoder.layer.10.attention.self.query_global.bias', 'encoder.layer.10.attention.self.key_global.weight', 'encoder.layer.10.attention.self.key_global.bias', 'encoder.layer.10.attention.self.value_global.weight', 'encoder.layer.10.attention.self.value_global.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.query_global.weight', 'encoder.layer.11.attention.self.query_global.bias', 'encoder.layer.11.attention.self.key_global.weight', 'encoder.layer.11.attention.self.key_global.bias', 'encoder.layer.11.attention.self.value_global.weight', 'encoder.layer.11.attention.self.value_global.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFLongformerModel: ['model.decoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.11.encoder_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.8.fc2.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.1.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.7.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.10.encoder_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.8.fc2.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.output.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.6.encoder_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.value.weight', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.encoder_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.6.fc2.weight', 'model.decoder.layers.1.fc2.weight', 'model.encoder.layers.3.self_attn.output.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.10.fc2.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.output.bias', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.4.fc1.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.8.self_attn.output.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.9.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.output.bias', 'model.decoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.11.fc2.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.10.final_layer_norm.weight', 'final_logits_bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.encoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.0.fc1.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.encoder.layers.5.fc1.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.11.encoder_attn.k_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.11.fc2.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.output.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.9.encoder_attn.k_proj.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.output.weight', 'model.encoder.layers.8.fc2.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.output.weight', 'model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.10.fc2.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.11.fc2.weight', 'model.decoder.layers.3.fc2.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.7.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.1.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.encoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.embed_positions.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.11.encoder_attn.q_proj.weight', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.3.fc2.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.value.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.0.fc2.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.9.fc2.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.6.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.7.fc1.weight', 'model.encoder.layers.11.fc1.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.7.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.output.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.10.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.9.encoder_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.3.fc2.bias', 'model.decoder.layers.9.encoder_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.10.fc2.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.10.self_attn.output.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.11.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.output.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.5.fc1.bias', 'model.decoder.layers.7.encoder_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.8.self_attn.output.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.8.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.10.encoder_attn.k_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.7.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.11.self_attn.output.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.9.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.10.encoder_attn.out_proj.weight', 'model.encoder.layers.11.fc1.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.11.fc1.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.output.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.8.encoder_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.7.fc2.bias', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.11.self_attn.longformer_self_attn.value.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.8.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.8.encoder_attn.out_proj.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.10.encoder_attn.k_proj.weight', 'model.encoder.layers.11.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.6.encoder_attn_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.3.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.output.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.6.encoder_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.7.fc1.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.output.weight', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc1.bias', 'model.decoder.layers.7.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.value_global.bias', 'model.decoder.layers.8.encoder_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.4.fc2.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.10.fc1.weight', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.query_global.bias', 'model.encoder.layers.9.fc1.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layernorm_embedding.weight', 'model.encoder.layers.8.fc2.bias', 'model.decoder.layers.6.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.8.fc1.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.8.encoder_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.output.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.11.encoder_attn.v_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.key.weight', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.decoder.layers.9.encoder_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.11.encoder_attn_layer_norm.weight', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.longformer_self_attn.value.bias', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.key_global.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.key.bias', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.10.self_attn.longformer_self_attn.query_global.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.query.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.bias', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.key.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.key_global.bias', 'model.decoder.layers.6.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.output.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.value_global.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.longformer_self_attn.value_global.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.longformer_self_attn.key.bias', 'model.encoder.layers.11.self_attn.longformer_self_attn.key_global.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.longformer_self_attn.query_global.weight', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.9.encoder_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.longformer_self_attn.query.weight', 'model.encoder.layers.11.self_attn.longformer_self_attn.value_global.weight', 'model.decoder.layers.10.encoder_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.query.bias', 'model.encoder.layers.1.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.9.fc2.weight', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.longformer_self_attn.query.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.key.weight', 'model.encoder.layers.7.self_attn.output.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.output.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.encoder.layers.2.self_attn.longformer_self_attn.value.weight', 'model.encoder.layers.9.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.4.self_attn.longformer_self_attn.value.weight', 'model.encoder.layers.8.fc1.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.9.fc2.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.output.weight', 'model.encoder.layers.7.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.output.weight', 'model.encoder.layers.2.fc1.bias', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.output.bias', 'model.decoder.layers.11.encoder_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.longformer_self_attn.key_global.bias', 'model.encoder.layers.8.self_attn.longformer_self_attn.value.bias', 'model.encoder.layers.5.self_attn.output.weight', 'model.shared.weight', 'model.encoder.layers.0.self_attn.longformer_self_attn.query_global.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.longformer_self_attn.value.weight', 'model.decoder.layers.7.fc2.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.4.fc2.bias', 'model.decoder.layernorm_embedding.bias', 'model.decoder.layers.8.encoder_attn.q_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing TFLongformerModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFLongformerModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.query_global.weight', 'encoder.layer.0.attention.self.query_global.bias', 'encoder.layer.0.attention.self.key_global.weight', 'encoder.layer.0.attention.self.key_global.bias', 'encoder.layer.0.attention.self.value_global.weight', 'encoder.layer.0.attention.self.value_global.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.query_global.weight', 'encoder.layer.1.attention.self.query_global.bias', 'encoder.layer.1.attention.self.key_global.weight', 'encoder.layer.1.attention.self.key_global.bias', 'encoder.layer.1.attention.self.value_global.weight', 'encoder.layer.1.attention.self.value_global.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.query_global.weight', 'encoder.layer.2.attention.self.query_global.bias', 'encoder.layer.2.attention.self.key_global.weight', 'encoder.layer.2.attention.self.key_global.bias', 'encoder.layer.2.attention.self.value_global.weight', 'encoder.layer.2.attention.self.value_global.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.query_global.weight', 'encoder.layer.3.attention.self.query_global.bias', 'encoder.layer.3.attention.self.key_global.weight', 'encoder.layer.3.attention.self.key_global.bias', 'encoder.layer.3.attention.self.value_global.weight', 'encoder.layer.3.attention.self.value_global.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.query_global.weight', 'encoder.layer.4.attention.self.query_global.bias', 'encoder.layer.4.attention.self.key_global.weight', 'encoder.layer.4.attention.self.key_global.bias', 'encoder.layer.4.attention.self.value_global.weight', 'encoder.layer.4.attention.self.value_global.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.query_global.weight', 'encoder.layer.5.attention.self.query_global.bias', 'encoder.layer.5.attention.self.key_global.weight', 'encoder.layer.5.attention.self.key_global.bias', 'encoder.layer.5.attention.self.value_global.weight', 'encoder.layer.5.attention.self.value_global.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query_global.weight', 'encoder.layer.6.attention.self.query_global.bias', 'encoder.layer.6.attention.self.key_global.weight', 'encoder.layer.6.attention.self.key_global.bias', 'encoder.layer.6.attention.self.value_global.weight', 'encoder.layer.6.attention.self.value_global.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.query_global.weight', 'encoder.layer.7.attention.self.query_global.bias', 'encoder.layer.7.attention.self.key_global.weight', 'encoder.layer.7.attention.self.key_global.bias', 'encoder.layer.7.attention.self.value_global.weight', 'encoder.layer.7.attention.self.value_global.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.query_global.weight', 'encoder.layer.8.attention.self.query_global.bias', 'encoder.layer.8.attention.self.key_global.weight', 'encoder.layer.8.attention.self.key_global.bias', 'encoder.layer.8.attention.self.value_global.weight', 'encoder.layer.8.attention.self.value_global.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.query_global.weight', 'encoder.layer.9.attention.self.query_global.bias', 'encoder.layer.9.attention.self.key_global.weight', 'encoder.layer.9.attention.self.key_global.bias', 'encoder.layer.9.attention.self.value_global.weight', 'encoder.layer.9.attention.self.value_global.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.query_global.weight', 'encoder.layer.10.attention.self.query_global.bias', 'encoder.layer.10.attention.self.key_global.weight', 'encoder.layer.10.attention.self.key_global.bias', 'encoder.layer.10.attention.self.value_global.weight', 'encoder.layer.10.attention.self.value_global.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.query_global.weight', 'encoder.layer.11.attention.self.query_global.bias', 'encoder.layer.11.attention.self.key_global.weight', 'encoder.layer.11.attention.self.key_global.bias', 'encoder.layer.11.attention.self.value_global.weight', 'encoder.layer.11.attention.self.value_global.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/msi/miniconda3/envs/longformer/lib/python3.11/site-packages/keras/src/layers/layer.py:1381: UserWarning: Layer 'dual_longformer_model' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling layer 'tf_longformer_model' (type TFLongformerModel).\n",
      "\n",
      "Data of type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n",
      "\n",
      "Call arguments received by layer 'tf_longformer_model' (type TFLongformerModel):\n",
      "  • input_ids=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=2/0)>\n",
      "  • attention_mask=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=2/0)>\n",
      "  • head_mask=None\n",
      "  • global_attention_mask=None\n",
      "  • token_type_ids=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=2/0)>\n",
      "  • position_ids=None\n",
      "  • inputs_embeds=None\n",
      "  • output_attentions=None\n",
      "  • output_hidden_states=None\n",
      "  • return_dict=None\n",
      "  • training=False''\n",
      "  warnings.warn(\n",
      "/home/msi/miniconda3/envs/longformer/lib/python3.11/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'dual_longformer_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Exception encountered when calling layer 'tf_longformer_model' (type TFLongformerModel).\n\nData of type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n\nCall arguments received by layer 'tf_longformer_model' (type TFLongformerModel):\n  • input_ids=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=1/0)>\n  • attention_mask=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=1/0)>\n  • head_mask=None\n  • global_attention_mask=None\n  • token_type_ids=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=1/0)>\n  • position_ids=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m dual_longformer()\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mSaveModelCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/longformer/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/longformer/lib/python3.11/site-packages/keras/src/trainers/trainer.py:1079\u001b[0m, in \u001b[0;36mTrainer._symbolic_build\u001b[0;34m(self, iterator, data_batch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mcompute_output_spec(\u001b[38;5;28mself\u001b[39m, x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1079\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to automatically build the model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1081\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease build it yourself before calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit/evaluate/predict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA model is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuilt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when its variables have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeen created and its `self.built` attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1085\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis True. Usually, calling the model on a batch \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof data is the right way to build it.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1087\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException encountered:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1088\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m     )\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_metrics_unbuilt:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;66;03m# Build all metric state with `backend.compute_output_spec`.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m     backend\u001b[38;5;241m.\u001b[39mcompute_output_spec(\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics,\n\u001b[1;32m   1094\u001b[0m         x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1098\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Exception encountered when calling layer 'tf_longformer_model' (type TFLongformerModel).\n\nData of type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n\nCall arguments received by layer 'tf_longformer_model' (type TFLongformerModel):\n  • input_ids=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=1/0)>\n  • attention_mask=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=1/0)>\n  • head_mask=None\n  • global_attention_mask=None\n  • token_type_ids=Traced<ShapedArray(int32[2,16000])>with<DynamicJaxprTrace(level=1/0)>\n  • position_ids=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = dual_longformer()\n",
    "history = model.fit(\n",
    "    x=input_train,\n",
    "    y=y_train,\n",
    "    epochs=3,\n",
    "    batch_size=2,\n",
    "    validation_data=(input_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[SaveModelCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m history\u001b[38;5;241m.\u001b[39mhistory:\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predict test data\n",
    "pred_test = np.argmax(model.predict(input_test), axis=1)\n",
    "\n",
    "print(classification_report(\n",
    "    [map_label[i] for i in y_test], \n",
    "    [map_label[i] for i in pred_test]\n",
    "))\n",
    "\n",
    "cnf_matrix = confusion_matrix(\n",
    "    [map_label[i] for i in y_test], \n",
    "    [map_label[i] for i in pred_test]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(map_label.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
